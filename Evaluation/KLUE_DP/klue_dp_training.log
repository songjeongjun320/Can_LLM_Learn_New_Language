2025-04-15 14:09:02,867 - __main__ - INFO - Total number of KLUE-DP labels: 64
2025-04-15 14:09:02,868 - __main__ - INFO - Starting KLUE-DP training and evaluation
2025-04-15 14:09:02,868 - __main__ - INFO - Processing model: Llama-3.2-3b-it
2025-04-15 14:09:02,870 - __main__ - INFO - Starting training for Llama-3.2-3b-it
2025-04-15 14:09:02,871 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-15 14:09:02,871 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/downloaded_models/Llama-3.2-3B-Instruct
2025-04-15 14:09:04,216 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-15 14:09:13,888 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-15 14:09:14,139 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-15 14:09:14,140 - __main__ - ERROR - Error processing Llama-3.2-3b-it: name 'MAX_TRAIN_SAMPLES' is not defined
2025-04-15 14:09:14,140 - __main__ - ERROR - Exception details:
Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 520, in <module>
    model, tokenizer = train_model(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 294, in train_model
    full_train_data[:MAX_TRAIN_SAMPLES],
                    ^^^^^^^^^^^^^^^^^
NameError: name 'MAX_TRAIN_SAMPLES' is not defined
2025-04-15 14:09:14,172 - __main__ - INFO - Processing model: Llama-3.1-8b-it
2025-04-15 14:09:14,174 - __main__ - INFO - Starting training for Llama-3.1-8b-it
2025-04-15 14:09:14,175 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-15 14:09:14,175 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/downloaded_models/Llama-3.1-8B-Instruct
2025-04-15 14:09:14,786 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-15 14:09:44,362 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-15 14:09:44,669 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-15 14:09:44,669 - __main__ - ERROR - Error processing Llama-3.1-8b-it: name 'MAX_TRAIN_SAMPLES' is not defined
2025-04-15 14:09:44,671 - __main__ - ERROR - Exception details:
Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 520, in <module>
    model, tokenizer = train_model(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 294, in train_model
    full_train_data[:MAX_TRAIN_SAMPLES],
                    ^^^^^^^^^^^^^^^^^
NameError: name 'MAX_TRAIN_SAMPLES' is not defined
2025-04-15 14:09:45,069 - __main__ - INFO - All results saved to: klue_dp_results/combined_results.json
2025-04-15 14:09:45,070 - __main__ - INFO - KLUE-DP training and evaluation completed
2025-04-15 14:13:29,924 - __main__ - INFO - Total number of KLUE-DP labels: 64
2025-04-15 14:13:29,934 - __main__ - INFO - Starting KLUE-DP training and evaluation
2025-04-15 14:13:29,935 - __main__ - INFO - Processing model: Llama-3.2-3b-it
2025-04-15 14:13:29,935 - __main__ - INFO - Starting training for Llama-3.2-3b-it
2025-04-15 14:13:29,936 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-15 14:13:29,936 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/downloaded_models/Llama-3.2-3B-Instruct
2025-04-15 14:13:30,845 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-15 14:13:37,406 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-15 14:13:37,636 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-15 14:13:46,609 - __main__ - ERROR - Error processing Llama-3.2-3b-it: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-04-15 14:13:46,610 - __main__ - ERROR - Exception details:
Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 520, in <module>
    model, tokenizer = train_model(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 307, in train_model
    training_args = TrainingArguments(
                    ^^^^^^^^^^^^^^^^^^
TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-04-15 14:13:46,675 - __main__ - INFO - Processing model: Llama-3.1-8b-it
2025-04-15 14:13:46,676 - __main__ - INFO - Starting training for Llama-3.1-8b-it
2025-04-15 14:13:46,677 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-15 14:13:46,677 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/downloaded_models/Llama-3.1-8B-Instruct
2025-04-15 14:13:47,214 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-15 14:14:10,248 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-15 14:14:10,437 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-15 14:14:19,129 - __main__ - ERROR - Error processing Llama-3.1-8b-it: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-04-15 14:14:19,129 - __main__ - ERROR - Exception details:
Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 520, in <module>
    model, tokenizer = train_model(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 307, in train_model
    training_args = TrainingArguments(
                    ^^^^^^^^^^^^^^^^^^
TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-04-15 14:14:19,292 - __main__ - INFO - All results saved to: klue_dp_results/combined_results.json
2025-04-15 14:14:19,293 - __main__ - INFO - KLUE-DP training and evaluation completed
2025-04-15 14:16:39,615 - __main__ - INFO - Total number of KLUE-DP labels: 64
2025-04-15 14:16:39,629 - __main__ - INFO - Starting KLUE-DP training and evaluation
2025-04-15 14:16:39,630 - __main__ - INFO - Processing model: Llama-3.2-3b-it
2025-04-15 14:16:39,630 - __main__ - INFO - Starting training for Llama-3.2-3b-it
2025-04-15 14:16:39,632 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-15 14:16:39,632 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/downloaded_models/Llama-3.2-3B-Instruct
2025-04-15 14:16:41,085 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-15 14:16:43,113 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-15 14:16:43,304 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-15 14:16:52,245 - __main__ - ERROR - Error processing Llama-3.2-3b-it: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: IntervalStrategy.NO
- Save strategy: SaveStrategy.STEPS
2025-04-15 14:16:52,272 - __main__ - ERROR - Exception details:
Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 520, in <module>
    model, tokenizer = train_model(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 307, in train_model
    training_args = TrainingArguments(
                    ^^^^^^^^^^^^^^^^^^
  File "<string>", line 132, in __init__
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/training_args.py", line 1648, in __post_init__
    raise ValueError(
ValueError: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: IntervalStrategy.NO
- Save strategy: SaveStrategy.STEPS
2025-04-15 14:16:52,341 - __main__ - INFO - Processing model: Llama-3.1-8b-it
2025-04-15 14:16:52,341 - __main__ - INFO - Starting training for Llama-3.1-8b-it
2025-04-15 14:16:52,342 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-15 14:16:52,342 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/downloaded_models/Llama-3.1-8B-Instruct
2025-04-15 14:16:52,636 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-15 14:17:11,285 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-15 14:17:11,463 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-15 14:17:20,389 - __main__ - ERROR - Error processing Llama-3.1-8b-it: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: IntervalStrategy.NO
- Save strategy: SaveStrategy.STEPS
2025-04-15 14:17:20,394 - __main__ - ERROR - Exception details:
Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 520, in <module>
    model, tokenizer = train_model(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 307, in train_model
    training_args = TrainingArguments(
                    ^^^^^^^^^^^^^^^^^^
  File "<string>", line 132, in __init__
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/training_args.py", line 1648, in __post_init__
    raise ValueError(
ValueError: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: IntervalStrategy.NO
- Save strategy: SaveStrategy.STEPS
2025-04-15 14:17:20,535 - __main__ - INFO - All results saved to: klue_dp_results/combined_results.json
2025-04-15 14:17:20,535 - __main__ - INFO - KLUE-DP training and evaluation completed
2025-04-15 14:20:14,151 - __main__ - INFO - Total number of KLUE-DP labels: 64
2025-04-15 14:20:14,167 - __main__ - INFO - Starting KLUE-DP training and evaluation
2025-04-15 14:20:14,167 - __main__ - INFO - Processing model: Llama-3.2-3b-it
2025-04-15 14:20:14,168 - __main__ - INFO - Starting training for Llama-3.2-3b-it
2025-04-15 14:20:14,169 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-15 14:20:14,169 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/downloaded_models/Llama-3.2-3B-Instruct
2025-04-15 14:20:15,625 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-15 14:20:17,359 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-15 14:20:17,544 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-15 14:20:26,475 - __main__ - ERROR - Error processing Llama-3.2-3b-it: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-04-15 14:20:26,480 - __main__ - ERROR - Exception details:
Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 521, in <module>
    model, tokenizer = train_model(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 307, in train_model
    training_args = TrainingArguments(
                    ^^^^^^^^^^^^^^^^^^
TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-04-15 14:20:26,544 - __main__ - INFO - Processing model: Llama-3.1-8b-it
2025-04-15 14:20:26,545 - __main__ - INFO - Starting training for Llama-3.1-8b-it
2025-04-15 14:20:26,545 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-15 14:20:26,546 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/downloaded_models/Llama-3.1-8B-Instruct
2025-04-15 14:20:26,903 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-15 14:20:30,364 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-15 14:20:30,536 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-15 14:20:39,210 - __main__ - ERROR - Error processing Llama-3.1-8b-it: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-04-15 14:20:39,211 - __main__ - ERROR - Exception details:
Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 521, in <module>
    model, tokenizer = train_model(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 307, in train_model
    training_args = TrainingArguments(
                    ^^^^^^^^^^^^^^^^^^
TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-04-15 14:20:39,563 - __main__ - INFO - All results saved to: klue_dp_results/combined_results.json
2025-04-15 14:20:39,564 - __main__ - INFO - KLUE-DP training and evaluation completed
2025-04-15 14:37:57,009 - __main__ - INFO - Total number of KLUE-DP labels: 64
2025-04-15 14:37:57,068 - __main__ - INFO - Starting KLUE-DP training and evaluation
2025-04-15 14:37:57,074 - __main__ - INFO - Processing model: Llama-3.2-3b-it
2025-04-15 14:37:57,074 - __main__ - INFO - Starting training for Llama-3.2-3b-it
2025-04-15 14:37:57,075 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-15 14:37:57,075 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/downloaded_models/Llama-3.2-3B-Instruct
2025-04-15 14:37:59,386 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-15 14:38:01,080 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-15 14:38:01,350 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-15 14:38:10,467 - __main__ - ERROR - Error processing Llama-3.2-3b-it: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-04-15 14:38:10,468 - __main__ - ERROR - Exception details:
Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 521, in <module>
    model, tokenizer = train_model(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 307, in train_model
    training_args = TrainingArguments(
                    ^^^^^^^^^^^^^^^^^^
TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-04-15 14:38:10,533 - __main__ - INFO - Processing model: Llama-3.1-8b-it
2025-04-15 14:38:10,534 - __main__ - INFO - Starting training for Llama-3.1-8b-it
2025-04-15 14:38:10,535 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-15 14:38:10,535 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/downloaded_models/Llama-3.1-8B-Instruct
2025-04-15 14:38:10,833 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-15 14:38:14,432 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-15 14:38:14,841 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-15 14:38:24,776 - __main__ - ERROR - Error processing Llama-3.1-8b-it: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-04-15 14:38:24,777 - __main__ - ERROR - Exception details:
Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 521, in <module>
    model, tokenizer = train_model(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 307, in train_model
    training_args = TrainingArguments(
                    ^^^^^^^^^^^^^^^^^^
TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-04-15 14:38:24,916 - __main__ - INFO - All results saved to: klue_dp_results/combined_results.json
2025-04-15 14:38:24,917 - __main__ - INFO - KLUE-DP training and evaluation completed
2025-04-15 14:42:14,460 - __main__ - INFO - Total number of KLUE-DP labels: 64
2025-04-15 14:42:14,525 - __main__ - INFO - Starting KLUE-DP training and evaluation
2025-04-15 14:42:14,526 - __main__ - INFO - Processing model: Llama-3.2-3b-it
2025-04-15 14:42:14,526 - __main__ - INFO - Starting training for Llama-3.2-3b-it
2025-04-15 14:42:14,527 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-15 14:42:14,527 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/downloaded_models/Llama-3.2-3B-Instruct
2025-04-15 14:42:15,864 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-15 14:42:17,438 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-15 14:42:17,622 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-15 14:42:26,389 - __main__ - INFO - Initializing trainer
2025-04-15 14:42:26,630 - accelerate.utils.other - WARNING - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-04-15 14:42:26,635 - __main__ - INFO - Starting training
2025-04-15 15:16:30,137 - __main__ - INFO - Saving final model to: klue_dp_results/lora-llama3.2-3b-it-klue-dp/final
2025-04-15 15:16:37,969 - __main__ - INFO - Training completed for Llama-3.2-3b-it
2025-04-15 15:16:38,013 - __main__ - INFO - Evaluating model: Llama-3.2-3b-it
2025-04-15 15:36:44,938 - __main__ - INFO - Evaluation results for Llama-3.2-3b-it:
2025-04-15 15:36:44,958 - __main__ - INFO - UAS: 0.6481
2025-04-15 15:36:44,958 - __main__ - INFO - LAS: 0.6356
2025-04-15 15:36:44,959 - __main__ - INFO - Total tokens evaluated: 3052
2025-04-15 15:36:45,022 - __main__ - INFO - Evaluation logs saved to: klue_dp_results/lora-llama3.2-3b-it-klue-dp/evaluation_log.json
2025-04-15 15:36:45,022 - __main__ - INFO - Evaluation results saved to: klue_dp_results/lora-llama3.2-3b-it-klue-dp/eval_results.json
2025-04-15 15:36:45,026 - __main__ - INFO - Completed processing for Llama-3.2-3b-it
2025-04-15 15:36:45,026 - __main__ - INFO - Processing model: Llama-3.1-8b-it
2025-04-15 15:36:45,027 - __main__ - INFO - Starting training for Llama-3.1-8b-it
2025-04-15 15:36:45,028 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-15 15:36:45,029 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/downloaded_models/Llama-3.1-8B-Instruct
2025-04-15 15:36:45,620 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-15 15:36:48,782 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-15 15:36:49,044 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-15 15:36:56,598 - __main__ - INFO - Initializing trainer
2025-04-15 15:36:56,606 - accelerate.utils.other - WARNING - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-04-15 15:36:56,612 - __main__ - INFO - Starting training
2025-04-15 15:36:59,640 - __main__ - ERROR - Error processing Llama-3.1-8b-it: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 23.50 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.64 GiB is allocated by PyTorch, and 80.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-15 15:36:59,641 - __main__ - ERROR - Exception details:
Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 521, in <module>
    model, tokenizer = train_model(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 352, in train_model
    trainer.train()
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/trainer.py", line 2611, in _inner_training_loop
    self.optimizer.step()
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/accelerate/optimizer.py", line 178, in step
    self.optimizer.step(closure)
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/torch/optim/adamw.py", line 243, in step
    adamw(
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/torch/optim/adamw.py", line 875, in adamw
    func(
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/torch/optim/adamw.py", line 699, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 23.50 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.64 GiB is allocated by PyTorch, and 80.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-15 15:36:59,928 - __main__ - INFO - All results saved to: klue_dp_results/combined_results.json
2025-04-15 15:36:59,929 - __main__ - INFO - KLUE-DP training and evaluation completed
2025-04-15 15:37:58,369 - __main__ - INFO - Total number of KLUE-DP labels: 64
2025-04-15 15:37:58,379 - __main__ - INFO - Starting KLUE-DP training and evaluation
2025-04-15 15:37:58,379 - __main__ - INFO - Processing model: Llama-3.1-8b-it
2025-04-15 15:37:58,380 - __main__ - INFO - Starting training for Llama-3.1-8b-it
2025-04-15 15:37:58,381 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-15 15:37:58,381 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/downloaded_models/Llama-3.1-8B-Instruct
2025-04-15 15:38:03,995 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-15 15:38:07,034 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-15 15:38:07,220 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-15 15:38:14,607 - __main__ - INFO - Initializing trainer
2025-04-15 15:38:14,622 - accelerate.utils.other - WARNING - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-04-15 15:38:14,628 - __main__ - INFO - Starting training
2025-04-15 15:38:18,276 - __main__ - ERROR - Error processing Llama-3.1-8b-it: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 77.73 GiB memory in use. Of the allocated memory 74.47 GiB is allocated by PyTorch, and 2.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-15 15:38:18,277 - __main__ - ERROR - Exception details:
Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 521, in <module>
    model, tokenizer = train_model(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 352, in train_model
    trainer.train()
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/trainer.py", line 3736, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/trainer.py", line 3801, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/utils/generic.py", line 965, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 841, in forward
    loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/loss/loss_utils.py", line 63, in ForCausalLMLoss
    loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/loss/loss_utils.py", line 35, in fixed_cross_entropy
    loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/torch/nn/functional.py", line 3494, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 77.73 GiB memory in use. Of the allocated memory 74.47 GiB is allocated by PyTorch, and 2.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-15 15:38:18,586 - __main__ - INFO - All results saved to: klue_dp_results/combined_results.json
2025-04-15 15:38:18,586 - __main__ - INFO - KLUE-DP training and evaluation completed
2025-04-15 15:39:55,034 - __main__ - INFO - Total number of KLUE-DP labels: 64
2025-04-15 15:39:55,035 - __main__ - INFO - Starting KLUE-DP training and evaluation
2025-04-15 15:39:55,035 - __main__ - INFO - Processing model: Llama-3.1-8b-it
2025-04-15 15:39:55,036 - __main__ - INFO - Starting training for Llama-3.1-8b-it
2025-04-15 15:39:55,036 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-15 15:39:55,037 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/downloaded_models/Llama-3.1-8B-Instruct
2025-04-15 15:39:56,150 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-15 15:39:59,075 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-15 15:39:59,260 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-15 15:40:06,630 - __main__ - INFO - Initializing trainer
2025-04-15 15:40:06,644 - accelerate.utils.other - WARNING - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-04-15 15:40:06,650 - __main__ - INFO - Starting training
2025-04-15 17:17:22,880 - __main__ - INFO - Saving final model to: klue_dp_results/lora-llama3.1-8b-it-klue-dp/final
2025-04-15 17:17:44,700 - __main__ - INFO - Training completed for Llama-3.1-8b-it
2025-04-15 17:17:44,742 - __main__ - INFO - Evaluating model: Llama-3.1-8b-it
2025-04-15 17:41:27,651 - __main__ - INFO - Evaluation results for Llama-3.1-8b-it:
2025-04-15 17:41:27,673 - __main__ - INFO - UAS: 0.5773
2025-04-15 17:41:27,674 - __main__ - INFO - LAS: 0.5662
2025-04-15 17:41:27,674 - __main__ - INFO - Total tokens evaluated: 3052
2025-04-15 17:41:28,125 - __main__ - INFO - Evaluation logs saved to: klue_dp_results/lora-llama3.1-8b-it-klue-dp/evaluation_log.json
2025-04-15 17:41:28,126 - __main__ - INFO - Evaluation results saved to: klue_dp_results/lora-llama3.1-8b-it-klue-dp/eval_results.json
2025-04-15 17:41:28,129 - __main__ - INFO - Completed processing for Llama-3.1-8b-it
2025-04-15 17:41:28,361 - __main__ - INFO - All results saved to: klue_dp_results/combined_results.json
2025-04-15 17:41:28,361 - __main__ - INFO - KLUE-DP training and evaluation completed
2025-04-16 11:12:06,944 - __main__ - INFO - Total number of KLUE-DP labels: 64
2025-04-16 11:12:06,960 - __main__ - INFO - Starting KLUE-DP training and evaluation
2025-04-16 11:12:06,961 - __main__ - INFO - Processing model: OLMo-1b-org
2025-04-16 11:12:06,963 - __main__ - INFO - Starting training for OLMo-1b-org
2025-04-16 11:12:06,963 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-16 11:12:06,964 - __main__ - INFO - Load model: allenai/OLMo-1B
2025-04-16 11:12:22,777 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-16 11:12:22,781 - accelerate.utils.modeling - WARNING - The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.
2025-04-16 11:12:29,603 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-16 11:12:29,880 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-16 11:12:38,601 - __main__ - INFO - Initializing trainer
2025-04-16 11:12:38,616 - accelerate.utils.other - WARNING - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-04-16 11:12:38,625 - __main__ - INFO - Starting training
2025-04-16 11:34:51,660 - __main__ - INFO - Saving final model to: klue_dp_results/olmo1B-org-klue-dp/final
2025-04-16 11:34:54,992 - __main__ - INFO - Training completed for OLMo-1b-org
2025-04-16 11:34:55,035 - __main__ - INFO - Evaluating model: OLMo-1b-org
2025-04-16 11:49:25,946 - __main__ - INFO - Evaluation results for OLMo-1b-org:
2025-04-16 11:49:26,041 - __main__ - INFO - UAS: 0.4663
2025-04-16 11:49:26,041 - __main__ - INFO - LAS: 0.4430
2025-04-16 11:49:26,042 - __main__ - INFO - Total tokens evaluated: 3052
2025-04-16 11:49:26,506 - __main__ - INFO - Evaluation logs saved to: klue_dp_results/olmo1B-org-klue-dp/evaluation_log.json
2025-04-16 11:49:26,507 - __main__ - INFO - Evaluation results saved to: klue_dp_results/olmo1B-org-klue-dp/eval_results.json
2025-04-16 11:49:26,510 - __main__ - INFO - Completed processing for OLMo-1b-org
2025-04-16 11:49:26,511 - __main__ - INFO - Processing model: OLMo-1b-Tuned
2025-04-16 11:49:26,512 - __main__ - INFO - Starting training for OLMo-1b-Tuned
2025-04-16 11:49:26,514 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-16 11:49:26,514 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/FineTuning/Fine_Tuned_Results/Full_olmo1B
2025-04-16 11:49:29,543 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-16 11:49:29,547 - accelerate.utils.modeling - WARNING - The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.
2025-04-16 11:49:36,289 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-16 11:49:36,975 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-16 11:49:45,647 - __main__ - INFO - Initializing trainer
2025-04-16 11:49:46,011 - accelerate.utils.other - WARNING - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-04-16 11:49:46,106 - __main__ - INFO - Starting training
2025-04-16 12:12:15,377 - __main__ - INFO - Saving final model to: klue_dp_results/olmo1B-v12-klue-dp/final
2025-04-16 12:12:18,722 - __main__ - INFO - Training completed for OLMo-1b-Tuned
2025-04-16 12:12:19,151 - __main__ - INFO - Evaluating model: OLMo-1b-Tuned
2025-04-16 12:26:53,298 - __main__ - INFO - Evaluation results for OLMo-1b-Tuned:
2025-04-16 12:26:53,345 - __main__ - INFO - UAS: 0.4469
2025-04-16 12:26:53,345 - __main__ - INFO - LAS: 0.4253
2025-04-16 12:26:53,346 - __main__ - INFO - Total tokens evaluated: 3052
2025-04-16 12:26:53,596 - __main__ - INFO - Evaluation logs saved to: klue_dp_results/olmo1B-v12-klue-dp/evaluation_log.json
2025-04-16 12:26:53,596 - __main__ - INFO - Evaluation results saved to: klue_dp_results/olmo1B-v12-klue-dp/eval_results.json
2025-04-16 12:26:53,600 - __main__ - INFO - Completed processing for OLMo-1b-Tuned
2025-04-16 12:26:53,600 - __main__ - INFO - Processing model: OLMo-7b-org
2025-04-16 12:26:53,603 - __main__ - INFO - Starting training for OLMo-7b-org
2025-04-16 12:26:53,618 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-16 12:26:53,618 - __main__ - INFO - Load model: allenai/OLMo-7B
2025-04-16 12:26:56,891 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-16 12:26:56,900 - accelerate.utils.modeling - WARNING - The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.
2025-04-16 12:27:40,161 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-16 12:27:40,643 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-16 12:27:49,848 - __main__ - INFO - Initializing trainer
2025-04-16 12:27:49,939 - accelerate.utils.other - WARNING - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-04-16 12:27:50,035 - __main__ - INFO - Starting training
2025-04-16 14:08:11,318 - __main__ - INFO - Saving final model to: klue_dp_results/olmo7B-org-klue-dp/final
2025-04-16 14:08:31,926 - __main__ - INFO - Training completed for OLMo-7b-org
2025-04-16 14:08:31,982 - __main__ - INFO - Evaluating model: OLMo-7b-org
2025-04-16 14:37:05,682 - __main__ - INFO - Evaluation results for OLMo-7b-org:
2025-04-16 14:37:05,779 - __main__ - INFO - UAS: 0.4774
2025-04-16 14:37:05,779 - __main__ - INFO - LAS: 0.4590
2025-04-16 14:37:05,780 - __main__ - INFO - Total tokens evaluated: 3052
2025-04-16 14:37:06,037 - __main__ - INFO - Evaluation logs saved to: klue_dp_results/olmo7B-org-klue-dp/evaluation_log.json
2025-04-16 14:37:06,038 - __main__ - INFO - Evaluation results saved to: klue_dp_results/olmo7B-org-klue-dp/eval_results.json
2025-04-16 14:37:06,041 - __main__ - INFO - Completed processing for OLMo-7b-org
2025-04-16 14:37:06,042 - __main__ - INFO - Processing model: OLMo-7b-Tuned
2025-04-16 14:37:06,043 - __main__ - INFO - Starting training for OLMo-7b-Tuned
2025-04-16 14:37:06,046 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-16 14:37:06,050 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/FineTuning/Fine_Tuned_Results/Full_olmo7B
2025-04-16 14:37:07,240 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-16 14:37:07,247 - accelerate.utils.modeling - WARNING - The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.
2025-04-16 14:37:29,297 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.
2025-04-16 14:37:29,298 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-16 14:37:29,662 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-16 14:37:38,546 - __main__ - INFO - Initializing trainer
2025-04-16 14:37:38,591 - accelerate.utils.other - WARNING - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-04-16 14:37:38,592 - accelerate.big_modeling - WARNING - You shouldn't move a model that is dispatched using accelerate hooks.
2025-04-16 14:37:38,592 - __main__ - ERROR - Error processing OLMo-7b-Tuned: You can't move a model that has some modules offloaded to cpu or disk.
2025-04-16 14:37:38,593 - __main__ - ERROR - Exception details:
Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 533, in <module>
    model, tokenizer = train_model(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 353, in train_model
    trainer = Trainer(
              ^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/trainer.py", line 614, in __init__
    self._move_model_to_device(model, args.device)
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/trainer.py", line 901, in _move_model_to_device
    model = model.to(device)
            ^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/accelerate/big_modeling.py", line 459, in wrapper
    raise RuntimeError("You can't move a model that has some modules offloaded to cpu or disk.")
RuntimeError: You can't move a model that has some modules offloaded to cpu or disk.
2025-04-16 14:37:38,710 - __main__ - INFO - Processing model: Llama-3.2:3B
2025-04-16 14:37:38,714 - __main__ - INFO - Starting training for Llama-3.2:3B
2025-04-16 14:37:38,715 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-16 14:37:38,716 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/llama3.2_3b
2025-04-16 14:37:38,716 - __main__ - ERROR - Error processing Llama-3.2:3B: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/scratch/jsong132/Can_LLM_Learn_New_Language/llama3.2_3b'. Use `repo_type` argument if needed.
2025-04-16 14:37:38,717 - __main__ - ERROR - Exception details:
Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/utils/hub.py", line 424, in cached_files
    hf_hub_download(
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/scratch/jsong132/Can_LLM_Learn_New_Language/llama3.2_3b'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 533, in <module>
    model, tokenizer = train_model(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 300, in train_model
    model, tokenizer = load_model_and_tokenizer(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 206, in load_model_and_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 946, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 778, in get_tokenizer_config
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/utils/hub.py", line 266, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/utils/hub.py", line 470, in cached_files
    resolved_files = [
                     ^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/utils/hub.py", line 471, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) for filename in full_filenames
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/utils/hub.py", line 134, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir=cache_dir, revision=revision)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/scratch/jsong132/Can_LLM_Learn_New_Language/llama3.2_3b'. Use `repo_type` argument if needed.
2025-04-16 14:37:39,068 - __main__ - INFO - All results saved to: klue_dp_results/combined_results.json
2025-04-16 14:37:39,069 - __main__ - INFO - KLUE-DP training and evaluation completed
2025-04-16 15:10:11,468 - __main__ - INFO - Total number of KLUE-DP labels: 64
2025-04-16 15:10:11,539 - __main__ - INFO - Starting KLUE-DP training and evaluation
2025-04-16 15:10:11,540 - __main__ - INFO - Processing model: OLMo-7b-Tuned
2025-04-16 15:10:11,540 - __main__ - INFO - Starting training for OLMo-7b-Tuned
2025-04-16 15:10:11,586 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-16 15:10:11,586 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/FineTuning/Fine_Tuned_Results/Full_olmo7B
2025-04-16 15:10:48,141 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-16 15:10:48,150 - accelerate.utils.modeling - WARNING - The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.
2025-04-16 15:11:06,069 - __main__ - INFO - Loading DP dataset from /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json
2025-04-16 15:11:06,430 - __main__ - INFO - Loaded 10000 samples for dependency parsing
2025-04-16 15:11:15,035 - __main__ - INFO - Initializing trainer
2025-04-16 15:11:15,780 - accelerate.utils.other - WARNING - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-04-16 15:11:15,786 - __main__ - INFO - Starting training
2025-04-16 16:51:12,419 - __main__ - INFO - Saving final model to: klue_dp_results/olmo7B-v13-klue-dp/final
2025-04-16 16:51:30,438 - __main__ - INFO - Training completed for OLMo-7b-Tuned
2025-04-16 16:51:30,481 - __main__ - INFO - Evaluating model: OLMo-7b-Tuned
2025-04-16 17:20:36,121 - __main__ - INFO - Evaluation results for OLMo-7b-Tuned:
2025-04-16 17:20:36,154 - __main__ - INFO - UAS: 0.4626
2025-04-16 17:20:36,154 - __main__ - INFO - LAS: 0.4407
2025-04-16 17:20:36,155 - __main__ - INFO - Total tokens evaluated: 3052
2025-04-16 17:20:36,240 - __main__ - INFO - Evaluation logs saved to: klue_dp_results/olmo7B-v13-klue-dp/evaluation_log.json
2025-04-16 17:20:36,240 - __main__ - INFO - Evaluation results saved to: klue_dp_results/olmo7B-v13-klue-dp/eval_results.json
2025-04-16 17:20:36,243 - __main__ - INFO - Completed processing for OLMo-7b-Tuned
2025-04-16 17:20:36,243 - __main__ - INFO - Processing model: Llama-3.2:3B
2025-04-16 17:20:36,244 - __main__ - INFO - Starting training for Llama-3.2:3B
2025-04-16 17:20:36,245 - __main__ - INFO - Dataset already exists: /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_train.json and /scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/klue_all_tasks_json/klue_dp_validation.json
2025-04-16 17:20:36,245 - __main__ - INFO - Load model: /scratch/jsong132/Can_LLM_Learn_New_Language/llama3.2_3b
2025-04-16 17:20:36,246 - __main__ - ERROR - Error processing Llama-3.2:3B: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/scratch/jsong132/Can_LLM_Learn_New_Language/llama3.2_3b'. Use `repo_type` argument if needed.
2025-04-16 17:20:36,246 - __main__ - ERROR - Exception details:
Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/utils/hub.py", line 424, in cached_files
    hf_hub_download(
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/scratch/jsong132/Can_LLM_Learn_New_Language/llama3.2_3b'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 533, in <module>
    model, tokenizer = train_model(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 300, in train_model
    model, tokenizer = load_model_and_tokenizer(model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/Evaluation/KLUE_DP/KLUE_DP_Full_eval.py", line 206, in load_model_and_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 946, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 778, in get_tokenizer_config
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/utils/hub.py", line 266, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/utils/hub.py", line 470, in cached_files
    resolved_files = [
                     ^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/utils/hub.py", line 471, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) for filename in full_filenames
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/transformers/utils/hub.py", line 134, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir=cache_dir, revision=revision)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/scratch/jsong132/Can_LLM_Learn_New_Language/venv/lib64/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/scratch/jsong132/Can_LLM_Learn_New_Language/llama3.2_3b'. Use `repo_type` argument if needed.
2025-04-16 17:20:36,478 - __main__ - INFO - All results saved to: klue_dp_results/combined_results.json
2025-04-16 17:20:36,479 - __main__ - INFO - KLUE-DP training and evaluation completed
