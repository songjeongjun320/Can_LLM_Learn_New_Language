{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OlmoForCausalLM(\n",
       "  (model): OlmoModel(\n",
       "    (embed_tokens): Embedding(50304, 4096, padding_idx=1)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x OlmoDecoderLayer(\n",
       "        (self_attn): OlmoAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): OlmoMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): OlmoLayerNorm()\n",
       "        (post_attention_layernorm): OlmoLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): OlmoLayerNorm()\n",
       "    (rotary_emb): OlmoRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# 테스트 데이터셋 로드 (KLUE STS)\n",
    "dataset = load_dataset('klue', 'sts')\n",
    "\n",
    "print(dataset[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Ollama 관련 설정 및 함수\n",
    "try:\n",
    "    import ollama\n",
    "    OLLAMA_AVAILABLE = True\n",
    "    # Ollama 서버 설정\n",
    "    ollama_host = \"http://sg016:11434\"\n",
    "    ollama_client = ollama.Client(host=ollama_host)\n",
    "except ImportError:\n",
    "    OLLAMA_AVAILABLE = False\n",
    "    print(\"Ollama 패키지가 설치되어 있지 않습니다. pip install ollama로 설치하세요.\")\n",
    "\n",
    "def check_ollama_server():\n",
    "    \"\"\"Ollama 서버 연결 상태 확인\"\"\"\n",
    "    if not OLLAMA_AVAILABLE:\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        response = requests.get(f\"{ollama_host}/api/version\")\n",
    "        print(f\"Ollama 서버 연결됨: 버전 {response.json().get('version', '알 수 없음')}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama 서버 연결 실패: {e}\")\n",
    "        return False\n",
    "\n",
    "# Hugging Face 모델 관련 함수\n",
    "def load_model_and_tokenizer(model_path, use_local=True, dtype=torch.bfloat16):\n",
    "    \"\"\"모델과 토크나이저를 로드하는 함수\"\"\"\n",
    "    print(f\"모델 로딩 중: {model_path}\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path, \n",
    "            local_files_only=use_local\n",
    "        )\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            local_files_only=use_local,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"모델 로딩 완료!\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"모델 로딩 중 오류 발생: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def predict_similarity_with_model(model, tokenizer, sentence1, sentence2):\n",
    "    \"\"\"특정 모델을 사용하여 두 문장의 유사도를 예측하는 함수\"\"\"\n",
    "    prompt = f\"Analyze the following sentence pair and provide a similarity score between 0 and 5, where 0 means no similarity and 5 means identical.\\n\\nSentence 1: {sentence1}\\nSentence 2: {sentence2}\\n\\nSimilarity Score:\"\n",
    "    \n",
    "    # 토크나이즈\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 추론 수행\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=15,\n",
    "            temperature=0.1,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 결과 디코딩\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 프롬프트 이후의 생성된 텍스트만 추출\n",
    "    response = response[len(prompt):]\n",
    "    \n",
    "    # 숫자 추출 시도\n",
    "    try:\n",
    "        # 0~5 사이의 숫자 찾기\n",
    "        match = re.search(r'([0-5](\\.\\d+)?)', response)\n",
    "        if match:\n",
    "            predicted_score = float(match.group(1))\n",
    "        else:\n",
    "            # 숫자를 찾지 못한 경우 기본값 설정\n",
    "            print(f\"점수 추출 불가: '{response}'. 기본값 사용.\")\n",
    "            predicted_score = 2.5  # 기본값\n",
    "    except Exception as e:\n",
    "        print(f\"점수 추출 오류: {e}. 응답: '{response}'\")\n",
    "        predicted_score = 2.5  # 오류 발생 시 기본값\n",
    "    \n",
    "    # 0~5 범위로 제한\n",
    "    predicted_score = max(0, min(5, predicted_score))\n",
    "    \n",
    "    return predicted_score\n",
    "\n",
    "def predict_similarity_ollama(sentence1, sentence2, model_name=\"llama3.2\", language=\"en\"):\n",
    "    \"\"\"Ollama API를 사용하여 두 문장의 유사도를 예측하는 함수\"\"\"\n",
    "    if not OLLAMA_AVAILABLE:\n",
    "        print(\"Ollama 패키지를 사용할 수 없습니다.\")\n",
    "        return 2.5\n",
    " \n",
    "    prompt = f\"Analyze the following sentence pair and provide a similarity score between 0 and 5, where 0 means no similarity and 5 means identical.\\n\\nSentence 1: {sentence1}\\nSentence 2: {sentence2}\\n\\nReply with only a number.\"\n",
    "    \n",
    "    try:\n",
    "        # 문장 쌍을 프롬프트와 함께 모델에 입력\n",
    "        response = ollama_client.chat(\n",
    "            model=model_name, \n",
    "            messages=[{\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "            }],\n",
    "            options={\n",
    "                'temperature': 0.1,  # 낮은 온도로 일관된 결과\n",
    "                'top_p': 0.95,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # 응답에서 점수 추출\n",
    "        content = response['message']['content'].strip()\n",
    "        \n",
    "        # 정규 표현식으로 숫자 추출\n",
    "        match = re.search(r'([0-5](\\.\\d+)?)', content)\n",
    "        if match:\n",
    "            predicted_score = float(match.group(1))\n",
    "        else:\n",
    "            # 숫자를 찾지 못한 경우 2.5로 기본 설정\n",
    "            print(f\"응답에서 점수를 추출할 수 없습니다: '{content}'. 기본값 사용.\")\n",
    "            predicted_score = 2.5\n",
    "            \n",
    "        # 0~5 범위로 제한\n",
    "        predicted_score = max(0, min(5, predicted_score))\n",
    "        \n",
    "        return predicted_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"예측 중 오류 발생: {e}\")\n",
    "        return 2.5  # 오류 발생 시 기본값\n",
    "\n",
    "# 통합 평가 함수\n",
    "def evaluate_model_on_klue_sts(model_path, eval_samples=100, save_results=True, output_dir=\"results\"):\n",
    "    \"\"\"KLUE STS 데이터셋을 사용하여 모델(HF 또는 Ollama)을 평가하는 함수\"\"\"\n",
    "    # 모델 정보 추출\n",
    "    model_type = model_info.get(\"type\", \"hf\")  # 기본값은 huggingface\n",
    "    model_name = model_info.get(\"name\", \"Unknown\")\n",
    "    language = model_info.get(\"language\", \"en\")\n",
    "    \n",
    "    if model_type == \"ollama\":\n",
    "        # Ollama 서버 확인\n",
    "        if not check_ollama_server():\n",
    "            print(\"Ollama 서버에 연결할 수 없어 평가를 건너뜁니다.\")\n",
    "            return None\n",
    "        model_path = model_info.get(\"path\", model_name)  # Ollama의 경우 path는 모델명과 동일\n",
    "    else:  # huggingface\n",
    "        model_path = model_info.get(\"path\")\n",
    "        use_local = model_info.get(\"local\", True)\n",
    "        \n",
    "        # 모델 및 토크나이저 로드 (HF 모델만)\n",
    "        model, tokenizer = load_model_and_tokenizer(model_path, use_local)\n",
    "        if model is None or tokenizer is None:\n",
    "            return None\n",
    "        \n",
    "        # 평가 모드로 전환\n",
    "        model.eval()\n",
    "    \n",
    "    # 데이터셋 로드\n",
    "    print(\"KLUE STS 데이터셋 로드 중...\")\n",
    "    dataset = load_dataset('klue', 'sts')\n",
    "    \n",
    "    # 예측 및 레이블 저장용 리스트\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    examples_data = []  # 디버깅 및 분석용 데이터 저장\n",
    "    \n",
    "    # 시작 시간 기록\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 평가할 샘플 수 설정\n",
    "    if eval_samples is None or eval_samples > len(dataset['validation']):\n",
    "        eval_samples = len(dataset['validation'])\n",
    "    \n",
    "    print(f\"총 {eval_samples}개 샘플 평가 시작...\")\n",
    "    for i, example in enumerate(dataset['validation']):\n",
    "        if i >= eval_samples:\n",
    "            break\n",
    "            \n",
    "        sentence1 = example['sentence1']\n",
    "        sentence2 = example['sentence2']\n",
    "        label = example['labels']['label']  # 실제 유사도 점수\n",
    "        \n",
    "        # 두 문장에 대해 유사도 예측 (모델 타입에 따라 다른 함수 사용)\n",
    "        if model_type == \"ollama\":\n",
    "            pred = predict_similarity_ollama(sentence1, sentence2, model_info[\"path\"])\n",
    "        else:  # huggingface\n",
    "            pred = predict_similarity_with_model(model, tokenizer, sentence1, sentence2)\n",
    "            \n",
    "        predictions.append(pred)\n",
    "        labels.append(label)\n",
    "        \n",
    "        # 예제 데이터 저장\n",
    "        examples_data.append({\n",
    "            'sentence1': sentence1,\n",
    "            'sentence2': sentence2,\n",
    "            'true_score': label,\n",
    "            'predicted_score': pred\n",
    "        })\n",
    "        \n",
    "        # 진행 상황 출력\n",
    "        if (i+1) % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_time = elapsed / (i+1)\n",
    "            remaining = avg_time * (eval_samples - (i+1))\n",
    "            print(f\"처리됨: {i+1}/{eval_samples} 예제 | 평균 시간: {avg_time:.2f}초/예제 | 남은 시간: {remaining/60:.2f}분\")\n",
    "    \n",
    "    # 평가 지표 계산\n",
    "    mse = mean_squared_error(labels, predictions)\n",
    "    mae = mean_absolute_error(labels, predictions)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"\\n평가 결과:\")\n",
    "    print(f\"모델: {model_name}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    \n",
    "    # 결과 저장\n",
    "    if save_results:\n",
    "        # 결과 디렉토리 생성\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # 파일명에 사용할 안전한 모델명 생성\n",
    "        safe_model_name = model_name.replace(\"/\", \"-\")\n",
    "        \n",
    "        # 결과 요약 저장\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'model_path': model_path,\n",
    "            'model_type': model_type,\n",
    "            'eval_samples': eval_samples,\n",
    "            'mse': float(mse),\n",
    "            'mae': float(mae),\n",
    "            'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        with open(f\"{output_dir}/{safe_model_name}_summary.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # 세부적인 예제별 결과 저장\n",
    "        df = pd.DataFrame(examples_data)\n",
    "        df.to_csv(f\"{output_dir}/{safe_model_name}_detailed.csv\", index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"결과가 {output_dir} 디렉토리에 저장되었습니다.\")\n",
    "    \n",
    "    return {\"mse\": mse, \"mae\": mae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OLMo-7B 평가 시작 =====\n",
      "모델 로딩 중: allenai/OLMo-7B-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09cdb588efd4f88992c827635032f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로딩 완료!\n",
      "KLUE STS 데이터셋 로드 중...\n",
      "총 519개 샘플 평가 시작...\n",
      "처리됨: 10/519 예제 | 평균 시간: 0.50초/예제 | 남은 시간: 4.28분\n",
      "처리됨: 20/519 예제 | 평균 시간: 0.49초/예제 | 남은 시간: 4.07분\n",
      "처리됨: 30/519 예제 | 평균 시간: 0.49초/예제 | 남은 시간: 3.96분\n",
      "처리됨: 40/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 3.87분\n",
      "처리됨: 50/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 3.78분\n",
      "처리됨: 60/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 3.70분\n",
      "처리됨: 70/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 3.61분\n",
      "처리됨: 80/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 3.53분\n",
      "처리됨: 90/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 3.43분\n",
      "처리됨: 100/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 3.35분\n",
      "처리됨: 110/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 3.27분\n",
      "처리됨: 120/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 3.19분\n",
      "처리됨: 130/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 3.11분\n",
      "처리됨: 140/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 3.02분\n",
      "처리됨: 150/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 2.94분\n",
      "처리됨: 160/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 2.86분\n",
      "처리됨: 170/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 2.79분\n",
      "처리됨: 180/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 2.72분\n",
      "처리됨: 190/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 2.64분\n",
      "처리됨: 200/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 2.56분\n",
      "처리됨: 210/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 2.47분\n",
      "처리됨: 220/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 2.39분\n",
      "처리됨: 230/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 2.31분\n",
      "처리됨: 240/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 2.23분\n",
      "처리됨: 250/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 2.15분\n",
      "처리됨: 260/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 2.07분\n",
      "처리됨: 270/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 1.99분\n",
      "처리됨: 280/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 1.91분\n",
      "처리됨: 290/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 1.83분\n",
      "처리됨: 300/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 1.75분\n",
      "처리됨: 310/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 1.67분\n",
      "처리됨: 320/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 1.59분\n",
      "처리됨: 330/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 1.51분\n",
      "처리됨: 340/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 1.43분\n",
      "처리됨: 350/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 1.35분\n",
      "처리됨: 360/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 1.27분\n",
      "처리됨: 370/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 1.19분\n",
      "처리됨: 380/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 1.11분\n",
      "처리됨: 390/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 1.03분\n",
      "처리됨: 400/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 0.95분\n",
      "처리됨: 410/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 0.87분\n",
      "처리됨: 420/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 0.79분\n",
      "처리됨: 430/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 0.71분\n",
      "처리됨: 440/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 0.63분\n",
      "처리됨: 450/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 0.55분\n",
      "처리됨: 460/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 0.47분\n",
      "처리됨: 470/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 0.39분\n",
      "처리됨: 480/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 0.31분\n",
      "처리됨: 490/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 0.23분\n",
      "처리됨: 500/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 0.15분\n",
      "처리됨: 510/519 예제 | 평균 시간: 0.48초/예제 | 남은 시간: 0.07분\n",
      "\n",
      "평가 결과:\n",
      "모델: OLMo-7B\n",
      "Mean Squared Error (MSE): 3.5466\n",
      "Mean Absolute Error (MAE): 1.5378\n",
      "결과가 klue_sts_results 디렉토리에 저장되었습니다.\n",
      "\n",
      "===== OLMo-7B-Fine-Tuned 평가 시작 =====\n",
      "모델 로딩 중: fine-tuned-models/fine-tuned-olmo7B-v12-80000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee3774dbcce483d8bd3a44c10b50a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 평가할 모델 경로 목록\n",
    "models_to_evaluate = [\n",
    "    {\n",
    "        \"type\": \"hf\",\n",
    "        \"path\": \"allenai/OLMo-7B-hf\",\n",
    "        \"name\": \"OLMo-7B\",\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"hf\",\n",
    "        \"path\": \"fine-tuned-models/fine-tuned-olmo7B-v12-80000\",\n",
    "        \"name\": \"OLMo-7B-Fine-Tuned\",\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"hf\",\n",
    "        \"path\": \"allenai/OLMo-1B-hf\",\n",
    "        \"name\": \"OLMo-1B\",\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"hf\",\n",
    "        \"path\": \"fine-tuned-models/fine-tuned-olmo1B-80000-v11\",\n",
    "        \"name\": \"OLMo-1B-Fine-Tuned\",\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"ollama\",\n",
    "        \"path\": \"llama3.2\",\n",
    "        \"name\": \"Llama-3.2\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# 결과 저장을 위한 디렉토리\n",
    "results_dir = \"klue_sts_results\"\n",
    "\n",
    "# 각 모델 평가\n",
    "all_results = {}\n",
    "for model_info in models_to_evaluate:\n",
    "    print(f\"\\n===== {model_info['name']} 평가 시작 =====\")\n",
    "    results = evaluate_model_on_klue_sts(\n",
    "        model_path=model_info[\"path\"],\n",
    "        eval_samples=len(dataset['validation']),  # 전체 검증 세트 사용\n",
    "        save_results=True,\n",
    "        output_dir=results_dir\n",
    "    )\n",
    "    if results:\n",
    "        all_results[model_info[\"name\"]] = results\n",
    "\n",
    "# 모든 모델 결과 비교\n",
    "if len(all_results) > 1:\n",
    "    print(\"\\n===== 모델 성능 비교 =====\")\n",
    "    for model_name, metrics in all_results.items():\n",
    "        print(f\"{model_name}: MSE={metrics['mse']:.4f}, MAE={metrics['mae']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49d399e022847adbc41e6503b10d52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/100 examples\n",
      "Processed 20/100 examples\n",
      "Processed 30/100 examples\n",
      "Processed 40/100 examples\n",
      "Processed 50/100 examples\n",
      "Processed 60/100 examples\n",
      "Processed 70/100 examples\n",
      "Processed 80/100 examples\n",
      "Processed 90/100 examples\n",
      "Processed 100/100 examples\n",
      "\n",
      "Evaluation Results:\n",
      "KLUE-STS | OLMo-7b\n",
      "Mean Squared Error (MSE): 3.9518\n",
      "Mean Absolute Error (MAE): 1.6060\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# 모델 경로 설정\n",
    "model_path = \"allenai/OLMo-7B-hf\"\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "\n",
    "def predict_similarity(sentence1, sentence2):\n",
    "    # 프롬프트 정의 (두 문장의 유사도 점수 평가 요청)\n",
    "    prompt = f\"Analyze the following sentence pair and provide a similarity score between 0 and 5, where 0 means no similarity and 5 means identical.\\n\\nSentence 1: {sentence1}\\nSentence 2: {sentence2}\\n\\nSimilarity Score:\"\n",
    "    \n",
    "    # 토크나이즈\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 추론 수행\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            temperature=0.1,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 결과 디코딩\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 프롬프트 이후의 생성된 텍스트만 추출\n",
    "    response = response[len(prompt):]\n",
    "    \n",
    "    # 숫자 추출 시도\n",
    "    try:\n",
    "        # 0~5 사이의 숫자 찾기\n",
    "        match = re.search(r'([0-5](\\.\\d+)?)', response)\n",
    "        if match:\n",
    "            predicted_score = float(match.group(1))\n",
    "        else:\n",
    "            # 숫자를 찾지 못한 경우 기본값 설정\n",
    "            print(f\"Unable to extract score from: '{response}'. Using default value.\")\n",
    "            predicted_score = 2.5  # 기본값\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting score: {e}. Response: '{response}'\")\n",
    "        predicted_score = 2.5  # 오류 발생 시 기본값\n",
    "    \n",
    "    # 0~5 범위로 제한\n",
    "    predicted_score = max(0, min(5, predicted_score))\n",
    "    \n",
    "    return predicted_score\n",
    "\n",
    "# 'validation' 데이터셋에서 예측 수행\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "# 평가 샘플 수 제한 (전체 데이터셋을 테스트하기에는 시간이 오래 걸릴 수 있음)\n",
    "eval_samples = 100  # 필요에 따라 조정\n",
    "\n",
    "for i, example in enumerate(dataset['validation']):\n",
    "    if i >= eval_samples:\n",
    "        break\n",
    "        \n",
    "    sentence1 = example['sentence1']\n",
    "    sentence2 = example['sentence2']\n",
    "    label = example['labels']['label']  # 실제 유사도 점수\n",
    "    \n",
    "    # 두 문장에 대해 유사도 예측\n",
    "    pred = predict_similarity(sentence1, sentence2)\n",
    "    predictions.append(pred)\n",
    "    labels.append(label)\n",
    "    \n",
    "    # 진행 상황 출력\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"Processed {i+1}/{eval_samples} examples\")\n",
    "\n",
    "# 평가 지표 계산 (MSE, MAE)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mse = mean_squared_error(labels, predictions)\n",
    "mae = mean_absolute_error(labels, predictions)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(\"KLUE-STS | OLMo-7b\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206b185726af41d98dbd88a54c6419e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation\n",
      "KLUE-STS | OLMo-7b fine tuned\n",
      "Mean Squared Error (MSE): 7.81635838150289\n",
      "Mean Absolute Error (MAE): 2.3836223506743734\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# KLUE STS, OLMo-7b FineTuned\n",
    "#####################################\n",
    "\n",
    "# 모델 경로 설정\n",
    "model_path = 'fine-tuned-models/fine-tuned-olmo7B-v12-80000'\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# 테스트 데이터셋 로드 (KLUE STS 예시, 필요에 따라 다른 데이터셋 사용)\n",
    "dataset = load_dataset('klue', 'sts')\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "# 프롬프트 정의 (두 문장의 유사도 점수 평가 요청)\n",
    "prompt = \"Analyze the following sentence pair and provide a similarity score between 0 and 5, where 0 means no similarity and 5 means identical.\"\n",
    "\n",
    "def predict_similarity(sentence1, sentence2):\n",
    "    # 문장 쌍을 프롬프트와 함께 모델에 입력\n",
    "    response = client.chat(\n",
    "        model='llama3.2', \n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': f\"{prompt} Sentence 1: {sentence1} Sentence 2: {sentence2}\",\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # 예측된 유사도 점수 추출 (응답에서 점수를 얻음)\n",
    "    predicted_score = float(response['message']['content'].strip())\n",
    "    \n",
    "    return predicted_score\n",
    "\n",
    "# 'validation' 데이터셋에서 예측 수행\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for example in dataset['validation']:\n",
    "    sentence1 = example['sentence1']\n",
    "    sentence2 = example['sentence2']\n",
    "    label = example['labels']['label']  # 'labels'에서 실제 값 가져오기\n",
    "    \n",
    "    # 두 문장에 대해 유사도 예측\n",
    "    pred = predict_similarity(sentence1, sentence2)\n",
    "    predictions.append(pred)\n",
    "    labels.append(label)\n",
    "\n",
    "# 평가 지표 계산 (MSE, MAE)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(labels, predictions)\n",
    "mae = mean_absolute_error(labels, predictions)\n",
    "\n",
    "print(\"Evaluation\")\n",
    "print(\"KLUE-STS | OLMo-7b fine tuned\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:\n",
      "KLUE-STS | OLMo-1b Original\n",
      "Mean Squared Error (MSE): 8.107687861271677\n",
      "Mean Absolute Error (MAE): 2.430635838150289\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# KLUE STS, OLMo-1b Original\n",
    "#####################################\n",
    "\n",
    "# 모델 경로 설정\n",
    "model_path = \"allenai/OLMo-1B-hf\"\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# 테스트 데이터셋 로드 (KLUE STS 예시, 필요에 따라 다른 데이터셋 사용)\n",
    "dataset = load_dataset('klue', 'sts')\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "# 프롬프트 정의 (두 문장의 유사도 점수 평가 요청)\n",
    "prompt = \"Analyze the following sentence pair and provide a similarity score between 0 and 5, where 0 means no similarity and 5 means identical.\"\n",
    "\n",
    "def predict_similarity(sentence1, sentence2):\n",
    "    # 문장 쌍을 프롬프트와 함께 모델에 입력\n",
    "    response = client.chat(\n",
    "        model='llama3.2', \n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': f\"{prompt} Sentence 1: {sentence1} Sentence 2: {sentence2}\",\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # 예측된 유사도 점수 추출 (응답에서 점수를 얻음)\n",
    "    predicted_score = float(response['message']['content'].strip())\n",
    "    \n",
    "    return predicted_score\n",
    "\n",
    "# 'validation' 데이터셋에서 예측 수행\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for example in dataset['validation']:\n",
    "    sentence1 = example['sentence1']\n",
    "    sentence2 = example['sentence2']\n",
    "    label = example['labels']['label']  # 'labels'에서 실제 값 가져오기\n",
    "    \n",
    "    # 두 문장에 대해 유사도 예측\n",
    "    pred = predict_similarity(sentence1, sentence2)\n",
    "    predictions.append(pred)\n",
    "    labels.append(label)\n",
    "\n",
    "# 평가 지표 계산 (MSE, MAE)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(labels, predictions)\n",
    "mae = mean_absolute_error(labels, predictions)\n",
    "\n",
    "\n",
    "print(\"Evaluation:\")\n",
    "print(\"KLUE-STS | OLMo-1b Original\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:\n",
      "KLUE-STS | OLMo-1b FineTuned\n",
      "Mean Squared Error (MSE): 7.588998073217726\n",
      "Mean Absolute Error (MAE): 2.340462427745665\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# KLUE STS, OLMo-1b FineTuned\n",
    "#####################################\n",
    "\n",
    "# 모델 경로 설정\n",
    "model_path = \"./fine-tuned-models/fine-tuned-olmo1B-80000-v11\"\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# 테스트 데이터셋 로드 (KLUE STS 예시, 필요에 따라 다른 데이터셋 사용)\n",
    "dataset = load_dataset('klue', 'sts')\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "# 프롬프트 정의 (두 문장의 유사도 점수 평가 요청)\n",
    "prompt = \"Analyze the following sentence pair and provide a similarity score between 0 and 5, where 0 means no similarity and 5 means identical.\"\n",
    "\n",
    "def predict_similarity(sentence1, sentence2):\n",
    "    # 문장 쌍을 프롬프트와 함께 모델에 입력\n",
    "    response = client.chat(\n",
    "        model='llama3.2', \n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': f\"{prompt} Sentence 1: {sentence1} Sentence 2: {sentence2}\",\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # 예측된 유사도 점수 추출 (응답에서 점수를 얻음)\n",
    "    predicted_score = float(response['message']['content'].strip())\n",
    "    \n",
    "    return predicted_score\n",
    "\n",
    "# 'validation' 데이터셋에서 예측 수행\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for example in dataset['validation']:\n",
    "    sentence1 = example['sentence1']\n",
    "    sentence2 = example['sentence2']\n",
    "    label = example['labels']['label']  # 'labels'에서 실제 값 가져오기\n",
    "    \n",
    "    # 두 문장에 대해 유사도 예측\n",
    "    pred = predict_similarity(sentence1, sentence2)\n",
    "    predictions.append(pred)\n",
    "    labels.append(label)\n",
    "\n",
    "# 평가 지표 계산 (MSE, MAE)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(labels, predictions)\n",
    "mae = mean_absolute_error(labels, predictions)\n",
    "\n",
    "\n",
    "print(\"Evaluation:\")\n",
    "print(\"KLUE-STS | OLMo-1b FineTuned\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server connected\n",
      "Ollama is running\n",
      "Evaluation\n",
      "KLUE-STS | Llama3.2\n",
      "Mean Squared Error (MSE): 3.6705973025048175\n",
      "Mean Absolute Error (MAE): 1.5129094412331405\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import ollama\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import re  # 정규 표현식 모듈 추가\n",
    "\n",
    "# Ollama 서버 설정\n",
    "ollama_host = \"http://sg001:11434\"\n",
    "client = ollama.Client(host=ollama_host)  # 클라이언트 인스턴스 생성\n",
    "\n",
    "# 서버 연결 확인\n",
    "try:\n",
    "    response = requests.get(ollama_host)\n",
    "    print(\"Server connected\")\n",
    "    print(response.text)\n",
    "except requests.ConnectionError:\n",
    "    print(\"Not connected\")\n",
    "\n",
    "# Llama3.2 모델을 사용하여 분석\n",
    "# Llama3.2 모델을 위한 프롬프트를 설정합니다\n",
    "prompt = \"Analyze the following sentence pair and provide a similarity score between 0 and 5, where 0 means no similarity and 5 means identical.\"\n",
    "\n",
    "# KLUE STS 데이터셋 로드\n",
    "dataset = load_dataset('klue', 'sts')\n",
    "\n",
    "# 프롬프트 정의 (두 문장의 유사도 점수 평가 요청)\n",
    "prompt = \"Analyze the following sentence pair and provide a similarity score between 0 and 5, where 0 means no similarity and 5 means identical.\"\n",
    "\n",
    "def predict_similarity(sentence1, sentence2):\n",
    "    # 문장 쌍을 프롬프트와 함께 모델에 입력\n",
    "    response = client.chat(\n",
    "        model='llama3.2', \n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': f\"{prompt} Sentence 1: {sentence1} Sentence 2: {sentence2}\",\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # 예측된 유사도 점수 추출 (응답에서 점수를 얻음)\n",
    "    predicted_score = float(response['message']['content'].strip())\n",
    "    \n",
    "    return predicted_score\n",
    "\n",
    "# 'validation' 데이터셋에서 예측 수행\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for example in dataset['validation']:\n",
    "    sentence1 = example['sentence1']\n",
    "    sentence2 = example['sentence2']\n",
    "    label = example['labels']['label']  # 'labels'에서 실제 값 가져오기\n",
    "    \n",
    "    # 두 문장에 대해 유사도 예측\n",
    "    pred = predict_similarity(sentence1, sentence2)\n",
    "    predictions.append(pred)\n",
    "    labels.append(label)\n",
    "\n",
    "# 평가 지표 계산 (MSE, MAE)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(labels, predictions)\n",
    "mae = mean_absolute_error(labels, predictions)\n",
    "\n",
    "print(\"Evaluation\")\n",
    "print(\"KLUE-STS | Llama3.2\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
