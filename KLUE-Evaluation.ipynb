{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076d60ceb6164cad9f19e95f1d3d3c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation\n",
      "KLUE-STS | OLMo-7b Original\n",
      "Mean Squared Error (MSE): 8.196705202312138\n",
      "Mean Absolute Error (MAE): 2.447976878612717\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# KLUE STS, OLMo-7b original\n",
    "#####################################\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# 모델 경로 설정\n",
    "model_path = \"allenai/OLMo-7B-hf\"\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# 테스트 데이터셋 로드 (KLUE STS 예시, 필요에 따라 다른 데이터셋 사용)\n",
    "dataset = load_dataset('klue', 'sts')\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "def predict_similarity(sentence1, sentence2):\n",
    "    # 두 문장을 결합하여 모델에 입력할 형식으로 준비\n",
    "    inputs = tokenizer(f\"Input: {sentence1}\\nOutput: {sentence2}\", return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "    inputs = inputs.to(model.device)  # 모델이 있는 디바이스로 이동\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 모델을 통해 예측 수행\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 마지막 단어에 대한 로짓 추출 (CausalLM 모델에서는 생성된 텍스트를 예측)\n",
    "    logits = outputs.logits  # 마지막 토큰의 로짓\n",
    "    predicted_token = torch.argmax(logits[0, -1, :]).item()  # 마지막 단어 예측\n",
    "    \n",
    "    # 예측된 토큰을 텍스트로 변환\n",
    "    predicted_word = tokenizer.decode(predicted_token)\n",
    "    \n",
    "    # 예측된 텍스트 길이를 유사도 점수로 변환 (임시 해결책)\n",
    "    predicted_score = len(predicted_word.split())  # 단어 수를 유사도 점수로 가정\n",
    "    \n",
    "    return predicted_score\n",
    "\n",
    "# 'validation' 데이터셋에서 예측 수행\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for example in dataset['validation']:\n",
    "    sentence1 = example['sentence1']\n",
    "    sentence2 = example['sentence2']\n",
    "    label = example['labels']['label']  # 'labels'에서 실제 값 가져오기\n",
    "    \n",
    "    pred = predict_similarity(sentence1, sentence2)\n",
    "    predictions.append(pred)\n",
    "    labels.append(label)\n",
    "\n",
    "# 평가 지표 계산 (예시: MSE, MAE)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(labels, predictions)\n",
    "mae = mean_absolute_error(labels, predictions)\n",
    "\n",
    "print(\"Evaluation\")\n",
    "print(\"KLUE-STS | OLMo-7b Original\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206b185726af41d98dbd88a54c6419e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation\n",
      "KLUE-STS | OLMo-7b fine tuned\n",
      "Mean Squared Error (MSE): 7.81635838150289\n",
      "Mean Absolute Error (MAE): 2.3836223506743734\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# KLUE STS, OLMo-7b FineTuned\n",
    "#####################################\n",
    "\n",
    "# 모델 경로 설정\n",
    "model_path = 'fine-tuned-models/fine-tuned-olmo7B-v12-80000'\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# 테스트 데이터셋 로드 (KLUE STS 예시, 필요에 따라 다른 데이터셋 사용)\n",
    "dataset = load_dataset('klue', 'sts')\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "def predict_similarity(sentence1, sentence2):\n",
    "    # 두 문장을 결합하여 모델에 입력할 형식으로 준비\n",
    "    inputs = tokenizer(f\"Input: {sentence1}\\nOutput: {sentence2}\", return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "    inputs = inputs.to(model.device)  # 모델이 있는 디바이스로 이동\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 모델을 통해 예측 수행\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 마지막 단어에 대한 로짓 추출 (CausalLM 모델에서는 생성된 텍스트를 예측)\n",
    "    logits = outputs.logits  # 마지막 토큰의 로짓\n",
    "    predicted_token = torch.argmax(logits[0, -1, :]).item()  # 마지막 단어 예측\n",
    "    \n",
    "    # 예측된 토큰을 텍스트로 변환\n",
    "    predicted_word = tokenizer.decode(predicted_token)\n",
    "    \n",
    "    # 예측된 텍스트 길이를 유사도 점수로 변환 (임시 해결책)\n",
    "    predicted_score = len(predicted_word.split())  # 단어 수를 유사도 점수로 가정\n",
    "    \n",
    "    return predicted_score\n",
    "\n",
    "# 'validation' 데이터셋에서 예측 수행\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for example in dataset['validation']:\n",
    "    sentence1 = example['sentence1']\n",
    "    sentence2 = example['sentence2']\n",
    "    label = example['labels']['label']  # 'labels'에서 실제 값 가져오기\n",
    "    \n",
    "    pred = predict_similarity(sentence1, sentence2)\n",
    "    predictions.append(pred)\n",
    "    labels.append(label)\n",
    "\n",
    "# 평가 지표 계산 (예시: MSE, MAE)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(labels, predictions)\n",
    "mae = mean_absolute_error(labels, predictions)\n",
    "\n",
    "print(\"Evaluation\")\n",
    "print(\"KLUE-STS | OLMo-7b fine tuned\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:\n",
      "KLUE-STS | OLMo-1b Original\n",
      "Mean Squared Error (MSE): 8.107687861271677\n",
      "Mean Absolute Error (MAE): 2.430635838150289\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# KLUE STS, OLMo-1b Original\n",
    "#####################################\n",
    "\n",
    "# 모델 경로 설정\n",
    "model_path = \"allenai/OLMo-1B-hf\"\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# 테스트 데이터셋 로드 (KLUE STS 예시, 필요에 따라 다른 데이터셋 사용)\n",
    "dataset = load_dataset('klue', 'sts')\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "# 'validation' 데이터셋에서 예측 수행\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for example in dataset['validation']:\n",
    "    sentence1 = example['sentence1']\n",
    "    sentence2 = example['sentence2']\n",
    "    label = example['labels']['label']  # 'labels'에서 실제 값 가져오기\n",
    "    \n",
    "    pred = predict_similarity(sentence1, sentence2)\n",
    "    predictions.append(pred)\n",
    "    labels.append(label)\n",
    "\n",
    "# 평가 지표 계산 (예시: MSE, MAE)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(labels, predictions)\n",
    "mae = mean_absolute_error(labels, predictions)\n",
    "\n",
    "print(\"Evaluation:\")\n",
    "print(\"KLUE-STS | OLMo-1b Original\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:\n",
      "KLUE-STS | OLMo-1b FineTuned\n",
      "Mean Squared Error (MSE): 7.588998073217726\n",
      "Mean Absolute Error (MAE): 2.340462427745665\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# KLUE STS, OLMo-1b FineTuned\n",
    "#####################################\n",
    "\n",
    "# 모델 경로 설정\n",
    "model_path = \"./fine-tuned-models/fine-tuned-olmo1B-80000-v11\"\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# 테스트 데이터셋 로드 (KLUE STS 예시, 필요에 따라 다른 데이터셋 사용)\n",
    "dataset = load_dataset('klue', 'sts')\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "# 'validation' 데이터셋에서 예측 수행\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for example in dataset['validation']:\n",
    "    sentence1 = example['sentence1']\n",
    "    sentence2 = example['sentence2']\n",
    "    label = example['labels']['label']  # 'labels'에서 실제 값 가져오기\n",
    "    \n",
    "    pred = predict_similarity(sentence1, sentence2)\n",
    "    predictions.append(pred)\n",
    "    labels.append(label)\n",
    "\n",
    "# 평가 지표 계산 (예시: MSE, MAE)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(labels, predictions)\n",
    "mae = mean_absolute_error(labels, predictions)\n",
    "\n",
    "print(\"Evaluation:\")\n",
    "print(\"KLUE-STS | OLMo-1b FineTuned\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server connected\n",
      "Ollama is running\n",
      "Evaluation\n",
      "KLUE-STS | Llama3.2\n",
      "Mean Squared Error (MSE): 3.6705973025048175\n",
      "Mean Absolute Error (MAE): 1.5129094412331405\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import ollama\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import re  # 정규 표현식 모듈 추가\n",
    "\n",
    "# Ollama 서버 설정\n",
    "ollama_host = \"http://sg001:11434\"\n",
    "client = ollama.Client(host=ollama_host)  # 클라이언트 인스턴스 생성\n",
    "\n",
    "# 서버 연결 확인\n",
    "try:\n",
    "    response = requests.get(ollama_host)\n",
    "    print(\"Server connected\")\n",
    "    print(response.text)\n",
    "except requests.ConnectionError:\n",
    "    print(\"Not connected\")\n",
    "\n",
    "# Llama3.2 모델을 사용하여 분석\n",
    "# Llama3.2 모델을 위한 프롬프트를 설정합니다\n",
    "prompt = \"Analyze the following sentence pair and provide a similarity score between 0 and 5, where 0 means no similarity and 5 means identical.\"\n",
    "\n",
    "# KLUE STS 데이터셋 로드\n",
    "dataset = load_dataset('klue', 'sts')\n",
    "\n",
    "# 예측 함수 정의\n",
    "def predict_similarity(sentence1, sentence2):\n",
    "    # Llama3.2 모델을 통해 유사도 점수를 예측\n",
    "    response = client.chat(\n",
    "        model='llama3.2', \n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': f\"{prompt} Sentence 1: {sentence1} Sentence 2: {sentence2}\",\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # 예측된 유사도 점수 추출 (정규 표현식을 사용하여 숫자만 추출)\n",
    "    result_text = response['message']['content'].strip()\n",
    "    \n",
    "    # 유사도 점수는 \"I would give a similarity score of X out of 5\" 형태로 반환되므로, 숫자만 추출\n",
    "    match = re.search(r'(\\d(\\.\\d+)?)\\s*out of 5', result_text)\n",
    "    if match:\n",
    "        predicted_score = float(match.group(1))  # 추출된 유사도 점수 반환\n",
    "    else:\n",
    "        predicted_score = 0.0  # 예기치 못한 형식이 나오면 기본값으로 0.0을 반환\n",
    "    \n",
    "    return predicted_score\n",
    "\n",
    "# 'validation' 데이터셋에서 예측 수행\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for example in dataset['validation']:\n",
    "    sentence1 = example['sentence1']\n",
    "    sentence2 = example['sentence2']\n",
    "    label = example['labels']['label']  # 'labels'에서 실제 값 가져오기\n",
    "    \n",
    "    pred = predict_similarity(sentence1, sentence2)\n",
    "    predictions.append(pred)\n",
    "    labels.append(label)\n",
    "\n",
    "# 평가 지표 계산 (예시: MSE, MAE)\n",
    "mse = mean_squared_error(labels, predictions)\n",
    "mae = mean_absolute_error(labels, predictions)\n",
    "\n",
    "print(\"Evaluation\")\n",
    "print(\"KLUE-STS | Llama3.2\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
