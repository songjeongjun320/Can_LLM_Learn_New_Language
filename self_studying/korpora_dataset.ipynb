{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Korpora\n",
      "  Using cached Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting dataclasses>=0.6 (from Korpora)\n",
      "  Using cached dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.18.0 (from Korpora)\n",
      "  Using cached numpy-2.2.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting tqdm>=4.46.0 (from Korpora)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting requests>=2.20.0 (from Korpora)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xlrd>=1.2.0 (from Korpora)\n",
      "  Using cached xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.20.0->Korpora)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.20.0->Korpora)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.20.0->Korpora)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.20.0->Korpora)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\songj\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.46.0->Korpora) (0.4.6)\n",
      "Using cached Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Using cached numpy-2.2.2-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: dataclasses, xlrd, urllib3, tqdm, numpy, idna, charset-normalizer, certifi, requests, Korpora\n",
      "Successfully installed Korpora-0.2.0 certifi-2025.1.31 charset-normalizer-3.4.1 dataclasses-0.6 idna-3.10 numpy-2.2.2 requests-2.32.3 tqdm-4.67.1 urllib3-2.3.0 xlrd-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kcbert': 'beomi@github 님이 만드신 KcBERT 학습데이터', 'korean_chatbot_data': 'songys@github 님이 만드신 챗봇 문답 데이터', 'korean_hate_speech': '{inmoonlight,warnikchow,beomi}@github 님이 만드신 혐오댓글데이터', 'korean_parallel_koen_news': 'jungyeul@github 님이 만드신 병렬 말뭉치', 'korean_petitions': 'lovit@github 님이 만드신 2017.08 ~ 2019.03 청와대 청원데이터', 'kornli': 'KakaoBrain 에서 제공하는 Natural Language Inference (NLI) 데이터', 'korsts': 'KakaoBrain 에서 제공하는 Semantic Textual Similarity (STS) 데이터', 'kowikitext': 'lovit@github 님이 만드신 wikitext 형식의 한국어 위키피디아 데이터', 'namuwikitext': 'lovit@github 님이 만드신 wikitext 형식의 나무위키 데이터', 'naver_changwon_ner': '네이버 + 창원대 NER shared task data', 'nsmc': 'e9t@github 님이 만드신 Naver sentiment movie corpus v1.0', 'question_pair': 'songys@github 님이 만드신 질문쌍(Paired Question v.2)', 'modu_news': '국립국어원에서 만든 모두의 말뭉치: 뉴스 말뭉치', 'modu_messenger': '국립국어원에서 만든 모두의 말뭉치: 메신저 말뭉치', 'modu_mp': '국립국어원에서 만든 모두의 말뭉치: 형태 분석 말뭉치', 'modu_ne': '국립국어원에서 만든 모두의 말뭉치: 개체명 분석 말뭉치', 'modu_spoken': '국립국어원에서 만든 모두의 말뭉치: 구어 말뭉치', 'modu_web': '국립국어원에서 만든 모두의 말뭉치: 웹 말뭉치', 'modu_written': '국립국어원에서 만든 모두의 말뭉치: 문어 말뭉치', 'open_subtitles': 'Open parallel corpus (OPUS) 에서 제공하는 영화 자막 번역 병렬 말뭉치', 'aihub_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (구어 + 대화 + 뉴스 + 한국문화 + 조례 + 지자체웹사이트)', 'aihub_spoken_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (구어)', 'aihub_conversation_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (대화)', 'aihub_news_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (뉴스)', 'aihub_korean_culture_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (한국문화)', 'aihub_decree_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (조례)', 'aihub_government_website_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (지자체웹사이트)'}\n"
     ]
    }
   ],
   "source": [
    "%pip install Korpora\n",
    "from Korpora import Korpora\n",
    "print(Korpora.corpus_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : beomi@github\n",
      "    Repository : https://github.com/Beomi/KcBERT/\n",
      "    References :\n",
      "\n",
      "    공개된 한국어 BERT는 대부분 한국어 위키, 뉴스 기사, 책 등 잘 정제된 데이터를 기반으로 학습한 모델입니다.\n",
      "\n",
      "    한편, 실제로 NSMC와 같은 댓글형 데이터셋은 정제되지 않았고 구어체 특징에 신조어가 많으며,\n",
      "    오탈자 등 공식적인 글쓰기에서 나타나지 않는 표현들이 빈번하게 등장합니다.\n",
      "\n",
      "    KcBERT는 위와 같은 특성의 데이터셋에 적용하기 위해, 네이버 뉴스에서 댓글과 대댓글을 수집해,\n",
      "    토크나이저와 BERT모델을 처음부터 학습한 Pretrained BERT 모델입니다.\n",
      "\n",
      "    KcBERT는 Huggingface의 Transformers 라이브러리를 통해 간편히 불러와 사용할 수 있습니다.\n",
      "    (별도의 파일 다운로드가 필요하지 않습니다.)\n",
      "\n",
      "    # License\n",
      "    MIT License\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[kcbert] download kcbert-train.tar.gzaa: 100%|██████████| 2.10G/2.10G [05:27<00:00, 6.41MB/s]   \n",
      "[kcbert] download kcbert-train.tar.gzab: 100%|██████████| 2.10G/2.10G [04:36<00:00, 7.58MB/s]   \n",
      "[kcbert] download kcbert-train.tar.gzac: 671MB [05:34, 2.01MB/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korpora does not support KcBERT fetch in Windows OS.Please open local directory C:\\Users\\songj/Korpora/ and unzip manually tar files\n"
     ]
    }
   ],
   "source": [
    "# corpus = Korpora.load(\"open_subtitles\")\n",
    "corpus = Korpora.load(\"kcbert\")\n",
    "print(corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
