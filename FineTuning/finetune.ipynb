{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:데이터 로딩 시작...\n",
      "INFO:__main__:47개 파일에서 총 81948개의 샘플을 로드했습니다.\n",
      "INFO:__main__:학습 데이터셋: 73753개, 검증 데이터셋: 8195개\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 폴더 경로 설정\n",
    "folder_path = \"./Data_Final_Reversed/\"\n",
    "\n",
    "# 모든 .json 파일을 읽어들여 데이터를 병합\n",
    "all_data = {\"input\": [], \"output\": []}\n",
    "logger.info(\"데이터 로딩 시작...\")\n",
    "file_count = 0\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_count += 1\n",
    "        with open(os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                all_data[\"input\"].extend([item[\"input\"] for item in data])\n",
    "                all_data[\"output\"].extend([item[\"output\"] for item in data])\n",
    "            except json.JSONDecodeError:\n",
    "                logger.error(f\"파일 읽기 오류: {filename}\")\n",
    "\n",
    "logger.info(f\"{file_count}개 파일에서 총 {len(all_data['input'])}개의 샘플을 로드했습니다.\")\n",
    "\n",
    "# 입력과 출력 길이가 맞는지 확인\n",
    "assert len(all_data[\"input\"]) == len(all_data[\"output\"]), \"입력과 출력 데이터 개수가 일치하지 않습니다.\"\n",
    "\n",
    "# Hugging Face Dataset으로 변환\n",
    "dataset = Dataset.from_dict(all_data)\n",
    "\n",
    "# 학습 및 검증 데이터셋 분할 (90% 학습, 10% 검증)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)  # 재현성을 위한 시드 설정\n",
    "logger.info(f\"학습 데이터셋: {len(dataset['train'])}개, 검증 데이터셋: {len(dataset['test'])}개\")\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "base_model = \"allenai/OLMo-7B\"\n",
    "logger.info(f\"모델 로드 중: {base_model}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "# 특수 토큰 확인 및 설정\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 메모리 효율을 위한 모델 로드 설정\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model, \n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"  # 자동으로 사용 가능한 GPU에 모델 분산\n",
    ")\n",
    "\n",
    "# 데이터 전처리 함수 (입력/출력 형식 수정)\n",
    "def preprocess_function(examples):\n",
    "    # 프롬프트 형식: \"Input: {input} Output: {output}\"\n",
    "    prompts = [f\"Input: {input}\\nOutput: \" for input in examples[\"input\"]]\n",
    "    inputs = tokenizer(prompts, truncation=True, max_length=1024, padding=False)\n",
    "    \n",
    "    # 출력 토큰화\n",
    "    outputs = tokenizer(examples[\"output\"], truncation=True, max_length=1024, padding=False)\n",
    "    \n",
    "    # 입력 ID와 출력 ID 결합\n",
    "    result = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(prompts)):\n",
    "        input_ids = inputs[\"input_ids\"][i]\n",
    "        output_ids = outputs[\"input_ids\"][i]\n",
    "        \n",
    "        # EOS 토큰 추가\n",
    "        if output_ids[-1] != tokenizer.eos_token_id:\n",
    "            output_ids.append(tokenizer.eos_token_id)\n",
    "        \n",
    "        # 입력과 출력 결합\n",
    "        combined_ids = input_ids + output_ids\n",
    "        attention_mask = [1] * len(combined_ids)\n",
    "        \n",
    "        # 최대 길이 제한\n",
    "        max_length = 1024  # 더 긴 컨텍스트 허용\n",
    "        if len(combined_ids) > max_length:\n",
    "            combined_ids = combined_ids[:max_length]\n",
    "            attention_mask = attention_mask[:max_length]\n",
    "        \n",
    "        result[\"input_ids\"].append(combined_ids)\n",
    "        result[\"attention_mask\"].append(attention_mask)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 데이터셋 전처리\n",
    "logger.info(\"데이터셋 토큰화 중...\")\n",
    "tokenized_train = dataset[\"train\"].map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    num_proc=4  # 병렬 처리\n",
    ")\n",
    "tokenized_eval = dataset[\"test\"].map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=dataset[\"test\"].column_names,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "# 학습 하이퍼파라미터 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"olmo-7b-finetuned\",\n",
    "    eval_strategy=\"steps\",  # evaluation_strategy 대신 eval_strategy 사용\n",
    "    eval_steps=500,\n",
    "    learning_rate=5e-5,  # 조정된 학습률\n",
    "    per_device_train_batch_size=4,  # GPU 메모리에 맞게 조정\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,  # 더 적은 배치 크기를 보완\n",
    "    num_train_epochs=3,  # 에폭 수 감소\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    fp16=False,\n",
    "    bf16=True,  # bfloat16 정밀도 사용\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,  # 워밍업 스텝 추가\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    load_best_model_at_end=True,  # 최고 성능 모델 로드\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"none\",  # tensorboard 제거, 로깅 비활성화\n",
    ")\n",
    "\n",
    "# 데이터 콜레이터 초기화\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# 체크포인트 경로 설정\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 트레이너 초기화 및 학습\n",
    "logger.info(\"트레이너 초기화 및 학습 시작...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 학습 실행\n",
    "trainer.train()\n",
    "\n",
    "# 최종 모델 및 토크나이저 저장\n",
    "final_model_path = \"fine-tuned-olmo7B-v12-80000\"\n",
    "logger.info(f\"최종 모델 저장 중: {final_model_path}\")\n",
    "model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "logger.info(\"파인튜닝 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 모델 테스트\n",
    "def test_model(prompt):\n",
    "    inputs = tokenizer(f\"Input: {prompt}\\nOutput: \", return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=300,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:프롬프트: 오늘 하루 어땠니?. Generally when this conversation happeneing? And what should I reply?\n",
      "INFO:__main__:응답: Input: 오늘 하루 어땠니?. Generally when this conversation happeneing? And what should I reply?\n",
      "Output: The image depicts a young woman with dark hair, wearing a white shirt, standing in front of a window. The purpose of the image is to showcase the woman's appearance and surroundings.\n",
      "\n",
      "* A young woman with dark hair:\n",
      "\t+ Her hair is styled in a neat and tidy manner.\n",
      "\t+ She has a subtle smile on her face.\n",
      "* She is wearing a white shirt:\n",
      "\t+ The shirt appears to be a casual, relaxed fit.\n",
      "\t+ It may be made of cotton or another lightweight material.\n",
      "* There is a window behind her:\n",
      "\t+ The window is large and lets in plenty of natural light.\n",
      "\t+ It provides a glimpse into the outside world.\n",
      "\n",
      "Overall, the image presents a serene and peaceful scene, with the woman's gentle smile and relaxed posture adding to the sense of calmness.\n",
      "Output: 되냐 그걸로 하면 다니까 그냥��100 오늘 뭐하는 날인데나 아니면��네 최소한 인간으로서의 예인은\n",
      "Output: The image features a young woman with long, straight black hair and bangs, wearing a white shirt. The\n",
      "\n",
      "\n",
      "INFO:__main__:프롬프트: 안녕 내 이름은 준이야. Generally when this conversation happeneing?  And what should I reply?\n",
      "INFO:__main__:응답: Input: 안녕 내 이름은 준이야. Generally when this conversation happeneing?  And what should I reply?\n",
      "Output: The image shows a screenshot from a TV show or movie, featuring three women sitting at a table with food in front of them. The purpose of the image is to capture a moment from the story being told.\n",
      "\n",
      "* Three women are sitting at a table:\n",
      "\t+ They appear to be eating or preparing to eat.\n",
      "\t+ Their facial expressions suggest they are engaged in conversation or discussion.\n",
      "* A plate of food is on the table:\n",
      "\t+ It appears to be some kind of snack or appetizer.\n",
      "\t+ The food is likely a part of the meal being shared among the three women.\n",
      "\n",
      "Overall, the image suggests that the three women are having a casual conversation or gathering, possibly over a snack or while enjoying a meal together.. 2 따다 니 이쪽 할게요 으 으 아� 우리 타임이 좀 있어고 하지만\n",
      "Output: This image features a screenshot from a TV show or movie, showcasing three women seated at a table with food in front of them.\n",
      "\n",
      "In the foreground, the woman on the left has blonde hair and is wearing a white hoodie. She appears to be speaking to her companions, who are both dressed in casual attire. The table is set with various dishes, including what looks like kimchi, rice,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 예시\n",
    "test_prompts = [\"오늘 하루 어땠니?. Generally when this conversation happeneing? And what should I reply?\", \"안녕 내 이름은 준이야. Generally when this conversation happeneing?  And what should I reply?\"]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    logger.info(f\"프롬프트: {prompt}\")\n",
    "    logger.info(f\"응답: {test_model(prompt)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제가 제공한 코드는 다음과 같은 방식으로 OLMo 모델을 한국어로 파인튜닝하려고 설계되었습니다:\n",
    "\n",
    "# 순차적 콘텍스트 학습: 이전 자막(subtitle1)과 그 자막이 등장하는 장면 설명(environment1), \n",
    "# 그리고 다음 자막(subtitle2)을 연결하여 모델이 한국어 자막과 시각적 환경 사이의 관계를 학습하도록 합니다.\n",
    "# 프롬프트 형식화: \"이전 한국어 자막 + 시각적 환경 설명 + 다음 한국어 자막 → 다음 자막의 시각적 환경\" \n",
    "# 형태로 학습 데이터를 구성합니다. 이를 통해 모델은 한국어 텍스트와 영어로 된 환경 설명 사이의 패턴을 인식할 수 있습니다.\n",
    "# 한국어 토크나이저 확장: OLMo의 토크나이저에 한글 문자를 추가하여 한국어 텍스트를 더 효율적으로 처리할 수 있도록 합니다.\n",
    "# LoRA 기반 효율적 파인튜닝: 전체 모델 파라미터를 업데이트하는 대신 LoRA(Low-Rank Adaptation)를 \n",
    "# 사용하여 적은 수의 파라미터만 훈련함으로써 계산 효율성을 높입니다.\n",
    "# 컨텍스트 기반 순차 학습: 드라마의 시간적 흐름을 따라 자막-환경 쌍을 순차적으로 학습함으로써 \n",
    "# 대화 흐름과 상황 변화에 따른 언어 사용 패턴을 파악합니다.\n",
    "\n",
    "# 간단히 말해, 이 코드는 한국어 자막과 그에 해당하는 영어 환경 설명을 연결하여 \n",
    "# OLMo가 한국어를 이해하고 적절한 환경 컨텍스트를 연결할 수 있도록 훈련하는 방식입니다.\n",
    "# ==> 이거는 이후에 테스트 해볼것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class KoreanVisualContextTrainer:\n",
    "    def __init__(self, base_path, model_name=\"allenai/OLMo-7B\", output_dir=\"./olmo-korean-finetuned\"):\n",
    "        self.base_path = base_path\n",
    "        self.model_name = model_name\n",
    "        self.output_dir = output_dir\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    def load_drama_data(self, drama_names, version=\"v2\"):\n",
    "        \"\"\"\n",
    "        Load drama data from multiple dramas\n",
    "        \n",
    "        Args:\n",
    "            drama_names: List of drama folder names\n",
    "            version: Version of the data\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing timestamped subtitle-description pairs\n",
    "        \"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        for drama in drama_names:\n",
    "            data_path = Path(f\"{self.base_path}/Refined_Datas/{version}/Data_Final/{drama}_final.json\")\n",
    "            if not data_path.exists():\n",
    "                logger.warning(f\"Data for {drama} not found at {data_path}\")\n",
    "                continue\n",
    "                \n",
    "            with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                drama_data = json.load(f)\n",
    "                \n",
    "            logger.info(f\"Loaded {len(drama_data)} samples from {drama}\")\n",
    "            all_data.extend(drama_data)\n",
    "            \n",
    "        logger.info(f\"Total samples loaded: {len(all_data)}\")\n",
    "        return all_data\n",
    "        \n",
    "    def prepare_sequential_data(self, data):\n",
    "        \"\"\"\n",
    "        Prepare sequential data for context-based training\n",
    "        \n",
    "        Args:\n",
    "            data: List of dictionaries with timestamp, input (subtitle), output (description)\n",
    "            \n",
    "        Returns:\n",
    "            List of training examples with context from previous turns\n",
    "        \"\"\"\n",
    "        processed_data = []\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        sorted_data = sorted(data, key=lambda x: x['timestamp'])\n",
    "        \n",
    "        for i in range(1, len(sorted_data)):\n",
    "            prev_sample = sorted_data[i-1]\n",
    "            curr_sample = sorted_data[i]\n",
    "            \n",
    "            # Create context-based training example\n",
    "            example = {\n",
    "                \"prev_subtitle\": prev_sample[\"input\"],\n",
    "                \"prev_description\": prev_sample[\"output\"],\n",
    "                \"current_subtitle\": curr_sample[\"input\"],\n",
    "                \"current_description\": curr_sample[\"output\"]\n",
    "            }\n",
    "            \n",
    "            processed_data.append(example)\n",
    "            \n",
    "        logger.info(f\"Created {len(processed_data)} sequential training examples\")\n",
    "        return processed_data\n",
    "        \n",
    "    def format_prompt(self, example):\n",
    "        \"\"\"\n",
    "        Format the prompt for training\n",
    "        \n",
    "        Args:\n",
    "            example: Dictionary with previous and current subtitles and descriptions\n",
    "            \n",
    "        Returns:\n",
    "            Formatted prompt and completion\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"The following is a conversation in Korean. I'll provide the previous subtitle, \n",
    "its visual context, and the next subtitle. Learn the relationship between the Korean text and its context.\n",
    "\n",
    "Previous Korean subtitle: {example['prev_subtitle']}\n",
    "Visual context: {example['prev_description']}\n",
    "Next Korean subtitle: {example['current_subtitle']}\n",
    "\n",
    "The visual context for the next subtitle is:\"\"\"\n",
    "        \n",
    "        completion = f\" {example['current_description']}\"\n",
    "        \n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"completion\": completion,\n",
    "            \"text\": f\"{prompt}{completion}\"\n",
    "        }\n",
    "        \n",
    "    def prepare_dataset(self, sequential_data):\n",
    "        \"\"\"\n",
    "        Prepare dataset for training\n",
    "        \n",
    "        Args:\n",
    "            sequential_data: List of sequential training examples\n",
    "            \n",
    "        Returns:\n",
    "            Dataset split into train and validation\n",
    "        \"\"\"\n",
    "        formatted_data = [self.format_prompt(example) for example in sequential_data]\n",
    "        \n",
    "        # Split into train and validation\n",
    "        train_data, val_data = train_test_split(formatted_data, test_size=0.1, random_state=42)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = Dataset.from_list(train_data)\n",
    "        val_dataset = Dataset.from_list(val_data)\n",
    "        \n",
    "        dataset_dict = DatasetDict({\n",
    "            'train': train_dataset,\n",
    "            'validation': val_dataset\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Train dataset size: {len(train_dataset)}\")\n",
    "        logger.info(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "        \n",
    "        return dataset_dict\n",
    "        \n",
    "    def initialize_model(self):\n",
    "        \"\"\"Initialize the model and tokenizer\"\"\"\n",
    "        logger.info(f\"Initializing model: {self.model_name}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Add special tokens for Korean if needed\n",
    "        # This is a simplified approach; more sophisticated tokenizer extension might be needed\n",
    "        # for optimal Korean language handling\n",
    "        korean_chars = set()\n",
    "        for i in range(0xAC00, 0xD7A4):  # Hangul syllables Unicode range\n",
    "            korean_chars.add(chr(i))\n",
    "        \n",
    "        new_tokens = list(korean_chars)\n",
    "        if new_tokens:\n",
    "            logger.info(f\"Adding {len(new_tokens)} Korean characters to tokenizer\")\n",
    "            self.tokenizer.add_tokens(new_tokens)\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Resize embeddings if new tokens were added\n",
    "        if new_tokens:\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        # Configure LoRA for efficient fine-tuning\n",
    "        lora_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        logger.info(\"Model initialized with LoRA configuration\")\n",
    "        \n",
    "        return self.model, self.tokenizer\n",
    "        \n",
    "    def tokenize_dataset(self, dataset_dict):\n",
    "        \"\"\"Tokenize the dataset\"\"\"\n",
    "        def tokenize_function(examples):\n",
    "            return self.tokenizer(\n",
    "                examples[\"text\"], \n",
    "                padding=\"max_length\", \n",
    "                truncation=True, \n",
    "                max_length=512\n",
    "            )\n",
    "            \n",
    "        tokenized_datasets = dataset_dict.map(\n",
    "            tokenize_function, \n",
    "            batched=True, \n",
    "            remove_columns=[\"prompt\", \"completion\"]\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Datasets tokenized\")\n",
    "        return tokenized_datasets\n",
    "        \n",
    "    def train(self, tokenized_datasets, batch_size=8, num_epochs=3):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=100,\n",
    "            learning_rate=2e-5,\n",
    "            weight_decay=0.01,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=4,\n",
    "            num_train_epochs=num_epochs,\n",
    "            warmup_steps=500,\n",
    "            save_steps=100,\n",
    "            load_best_model_at_end=True,\n",
    "            logging_dir=f\"{self.output_dir}/logs\",\n",
    "            fp16=True,\n",
    "        )\n",
    "        \n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"validation\"],\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Starting training\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the model\n",
    "        self.model.save_pretrained(f\"{self.output_dir}/final\")\n",
    "        self.tokenizer.save_pretrained(f\"{self.output_dir}/final\")\n",
    "        logger.info(f\"Model saved to {self.output_dir}/final\")\n",
    "        \n",
    "        return trainer\n",
    "        \n",
    "    def evaluate(self, trainer, test_korean_sentences):\n",
    "        \"\"\"Evaluate the model on test sentences\"\"\"\n",
    "        logger.info(\"Running evaluation\")\n",
    "        \n",
    "        results = []\n",
    "        for sentence in test_korean_sentences:\n",
    "            formatted_prompt = f\"The following is a Korean sentence. Describe the visual context in which this sentence might be spoken: {sentence}\"\n",
    "            inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            \n",
    "            output = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=200,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "            \n",
    "            response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            results.append({\n",
    "                \"input\": sentence,\n",
    "                \"output\": response.replace(formatted_prompt, \"\").strip()\n",
    "            })\n",
    "            \n",
    "        # Save results\n",
    "        with open(f\"{self.output_dir}/evaluation_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        logger.info(f\"Evaluation results saved to {self.output_dir}/evaluation_results.json\")\n",
    "        return results\n",
    "        \n",
    "    def run_pipeline(self, drama_names, test_sentences, version=\"v2\", batch_size=8, num_epochs=3):\n",
    "        \"\"\"Run the full pipeline\"\"\"\n",
    "        # Load data\n",
    "        data = self.load_drama_data(drama_names, version)\n",
    "        \n",
    "        # Prepare sequential data\n",
    "        sequential_data = self.prepare_sequential_data(data)\n",
    "        \n",
    "        # Prepare dataset\n",
    "        dataset_dict = self.prepare_dataset(sequential_data)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.initialize_model()\n",
    "        \n",
    "        # Tokenize dataset\n",
    "        tokenized_datasets = self.tokenize_dataset(dataset_dict)\n",
    "        \n",
    "        # Train model\n",
    "        trainer = self.train(tokenized_datasets, batch_size, num_epochs)\n",
    "        \n",
    "        # Evaluate model\n",
    "        evaluation_results = self.evaluate(trainer, test_sentences)\n",
    "        \n",
    "        return evaluation_results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = \"/path/to/your/data\"\n",
    "    drama_names = [\"house_of_cards\", \"friends\", \"breaking_bad\"]  # Add your drama names\n",
    "    test_sentences = [\n",
    "        \"이게 무슨 일이야?\",\n",
    "        \"나중에 보자.\",\n",
    "        \"어디 가고 싶어?\",\n",
    "        \"정말 미안해요.\",\n",
    "        \"배고파요.\"\n",
    "    ]\n",
    "    \n",
    "    trainer = KoreanVisualContextTrainer(\n",
    "        base_path=base_path,\n",
    "        model_name=\"allenai/OLMo-7B\",\n",
    "        output_dir=\"./olmo-korean-visual-context\"\n",
    "    )\n",
    "    \n",
    "    results = trainer.run_pipeline(\n",
    "        drama_names=drama_names,\n",
    "        test_sentences=test_sentences,\n",
    "        batch_size=8,\n",
    "        num_epochs=3\n",
    "    )\n",
    "    \n",
    "    print(\"Fine-tuning completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
